<!doctype html><html><head><link rel=stylesheet href=https://hybrid-cloud-patterns.io/sass/patternfly.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://hybrid-cloud-patterns.io/sass/patternfly-addons.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/solid.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/fontawesome.css><title>OpenShift General Sizing | Hybrid Cloud Patterns</title></head><body><div class=pf-c-page><header class=pf-c-page__header><div class=pf-c-page__header-brand><a href=/ class=pf-c-page__header-brand-link><img src=/images/hybrid_cloud_patterns.png alt="Hybrid Cloud Patterns"></a></div><div class=pf-c-page__header-tools><nav class="pf-c-nav pf-m-horizontal" role=navigation><ul class=pf-c-nav__list><li class=pf-c-nav__item><a class=pf-c-nav__link href=/patterns/ title=Patterns>Patterns</a></li><li class="pf-c-nav__item pf-m-current"><a class=pf-c-nav__link href=/learn/ title=Learn>Learn</a></li><li class=pf-c-nav__item><a class=pf-c-nav__link href=/contribute/ title=Contribute>Contribute</a></li><li class=pf-c-nav__item><a class=pf-c-nav__link href=/blog/ title=Blog>Blog</a></li></ul></nav></div></header><div class=pf-c-page__sidebar><div class=pf-c-page__sidebar-body><div class=pf-c-nav><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/about/ class=pf-c-nav__link>Hybrid Cloud Patterns</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/quickstart/ class=pf-c-nav__link>Patterns quick start</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/infrastructure/ class=pf-c-nav__link>Infrastructure</a><section class=pf-c-nav__subnav><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/ocp-cluster-general-sizing/ class="pf-c-nav__link pf-m-current">OpenShift General Sizing</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/rhel-for-edge-general-sizing/ class=pf-c-nav__link>RHEL for Edge General Sizing</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/using-validated-pattern-operator/ class=pf-c-nav__link>Using the Validated Pattern operator</a></li></ul></section></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/workflow/ class=pf-c-nav__link>Workflow</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/secrets/ class=pf-c-nav__link>Secrets</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=/learn/faq/ class=pf-c-nav__link>FAQ</a></li></ul></div></div></div><main class=pf-c-page__main><section class=pf-c-page__main-section><div class="pf-l-grid pf-m-gutter"><div class="pf-l-grid__item pf-m-8-col"><div class=pf-c-content><div class=pf-c-content><h1 id=openshift-general-sizing>OpenShift General Sizing</h1><h2 id=recommended-node-host-practices>Recommended node host practices</h2><p>The OpenShift Container Platform node configuration file contains important options. For example, two parameters control the maximum number of pods that can be scheduled to a node: <code>podsPerCore</code> and <code>maxPods</code>.</p><p>When both options are in use, the lower of the two values limits the number of pods on a node. Exceeding these values can result in:</p><ul><li>Increased CPU utilization.</li><li>Slow pod scheduling.</li><li>Potential out-of-memory scenarios, depending on the amount of memory in the node.</li><li>Exhausting the pool of IP addresses.</li><li>Resource overcommitting, leading to poor user application performance.</li></ul><p>In Kubernetes, a pod that is holding a single container actually uses two containers. The second container is used to set up networking prior to the actual container starting. Therefore, a system running 10 pods will actually have 20 containers running.</p><p><strong>podsPerCore</strong> sets the number of pods the node can run based on the number of processor cores on the node. For example, if <strong>podsPerCore</strong> is set to <code>10</code> on a node with 4 processor cores, the maximum number of pods allowed on the node will be <code>40</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>kubeletConfig</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>podsPerCore</span>: <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><p>Setting <strong>podsPerCore</strong> to <code>0</code> disables this limit. The default is <code>0</code>. <strong>podsPerCore</strong> cannot exceed <code>maxPods</code>.</p><p><strong>maxPods</strong> sets the number of pods the node can run to a fixed value, regardless of the properties of the node.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span> <span style=color:#f92672>kubeletConfig</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>maxPods</span>: <span style=color:#ae81ff>250</span>
</span></span></code></pre></div><p>For more information about sizing and Red Hat standard host practices see the <a href=https://docs.openshift.com/container-platform/4.8/scalability_and_performance/recommended-host-practices.html>Official OpenShift Documentation Page</a> for recommended host practices.</p><h2 id=control-plane-node-sizing>Control plane node sizing</h2><p>The control plane node resource requirements depend on the number of nodes in the cluster. The following control plane node size recommendations are based on the results of control plane density focused testing. The control plane tests create the following objects across the cluster in each of the namespaces depending on the node counts:</p><ul><li>12 image streams</li><li>3 build configurations</li><li>6 builds</li><li>1 deployment with 2 pod replicas mounting two secrets each</li><li>2 deployments with 1 pod replica mounting two secrets</li><li>3 services pointing to the previous deployments</li><li>3 routes pointing to the previous deployments</li><li>10 secrets, 2 of which are mounted by the previous deployments</li><li>10 config maps, 2 of which are mounted by the previous deployments</li></ul><table><thead><tr><th style=text-align:left>Number of worker nodes</th><th style=text-align:left>Cluster load (namespaces)</th><th style=text-align:left>CPU cores</th><th style=text-align:left>Memory (GB)</th></tr></thead><tbody><tr><td style=text-align:left>25</td><td style=text-align:left>500</td><td style=text-align:left>4</td><td style=text-align:left>16</td></tr><tr><td style=text-align:left>100</td><td style=text-align:left>1000</td><td style=text-align:left>8</td><td style=text-align:left>32</td></tr><tr><td style=text-align:left>250</td><td style=text-align:left>4000</td><td style=text-align:left>16</td><td style=text-align:left>96</td></tr></tbody></table><p>On a cluster with three masters or control plane nodes, the CPU and memory usage will spike up when one of the nodes is stopped, rebooted or fails because the remaining two nodes must handle the load in order to be highly available. This is also expected during upgrades because the masters are cordoned, drained, and rebooted serially to apply the operating system updates, as well as the control plane Operators update. To avoid cascading failures on large and dense clusters, keep the overall resource usage on the master nodes to at most half of all available capacity to handle the resource usage spikes. Increase the CPU and memory on the master nodes accordingly.</p><p>The node sizing varies depending on the number of nodes and object counts in the cluster. It also depends on whether the objects are actively being created on the cluster. During object creation, the control plane is more active in terms of resource usage compared to when the objects are in the <code>running</code> phase.</p><p>If you used an installer-provisioned infrastructure installation method, you cannot modify the control plane node size in a running OpenShift Container Platform 4.5 cluster. Instead, you must estimate your total node count and use the suggested control plane node size during installation.</p><p>The recommendations are based on the data points captured on OpenShift Container Platform clusters with OpenShiftSDN as the network plug-in.</p><p>In OpenShift Container Platform 4.5, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. The sizes are determined taking that into consideration.</p><p>For more information about sizing and Red Hat standard host practices see the <a href=https://docs.openshift.com/container-platform/4.8/scalability_and_performance/recommended-host-practices.html>Official OpenShift Documentation Page</a> for recommended host practices.</p><h2 id=recommended-etcd-practices>Recommended etcd practices</h2><p>For large and dense clusters, etcd can suffer from poor performance if the keyspace grows excessively large and exceeds the space quota. Periodic maintenance of etcd, including defragmentation, must be performed to free up space in the data store. It is highly recommended that you monitor Prometheus for etcd metrics and defragment it when required before etcd raises a cluster-wide alarm that puts the cluster into a maintenance mode, which only accepts key reads and deletes. Some of the key metrics to monitor are <code>etcd_server_quota_backend_bytes</code> which is the current quota limit, <code>etcd_mvcc_db_total_size_in_use_in_bytes</code> which indicates the actual database usage after a history compaction, and <code>etcd_debugging_mvcc_db_total_size_in_bytes</code> which shows the database size including free space waiting for defragmentation. Instructions on defragging etcd can be found in the <code>Defragmenting etcd data</code> section.</p><p>Etcd writes data to disk, so its performance strongly depends on disk performance. Etcd persists proposals on disk. Slow disks and disk activity from other processes might cause long fsync latencies, causing etcd to miss heartbeats, inability to commit new proposals to the disk on time, which can cause request timeouts and temporary leader loss. It is highly recommended to run etcd on machines backed by SSD/NVMe disks with low latency and high throughput.</p><p>Some of the key metrics to monitor on a deployed OpenShift Container Platform cluster are p99 of etcd disk write ahead log duration and the number of etcd leader changes. Use Prometheus to track these metrics. <code>etcd_disk_wal_fsync_duration_seconds_bucket</code> reports the etcd disk fsync duration, <code>etcd_server_leader_changes_seen_total</code> reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the <code>etcd_disk_wal_fsync_duration_seconds_bucket</code> should be less than 10ms.</p><p>For more information about sizing and Red Hat standard host practices see the <a href=https://docs.openshift.com/container-platform/4.8/scalability_and_performance/recommended-host-practices.html>Official OpenShift Documentation Page</a> for recommended host practices.</p></div></div></div><aside class="pf-l-grid__item pf-m-4-col pf-c-jump-links pf-m-vertical sticky pf-m-expandable pf-m-non-expandable-on-2xl" aria-label="Table of contents"><div class=pf-c-jump-links__header><div class=pf-c-jump-links__label><h1>Table of Contents</h1></div></div><nav id=TableOfContents><ul class=pf-c-jump-links__list><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#openshift-general-sizing>OpenShift General Sizing</a><ul class=pf-c-jump-links__list><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#recommended-node-host-practices>Recommended node host practices</a></li><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#control-plane-node-sizing>Control plane node sizing</a></li><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#recommended-etcd-practices>Recommended etcd practices</a></li></ul></li></ul></nav></aside></div></section><footer id=footer class="footer-dark pf-m-no-fill pf-l-flex footer-center"><div class=pf-l-flex__item><a href=//www.redhat.com target=top aria-label="Visit Red Hat.com"><img src=/images/RHlogo.svg alt="Red Hat logo" width=145px height=613px></a><span class=site-copyright>Copyright &copy; 2023 Red Hat, Inc.</span></div></footer></main></div></body></html>